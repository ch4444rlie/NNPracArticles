{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import ollama\n",
    "\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import ollama\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        #intialize weights and biases\n",
    "        self.weights = .01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        #set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        if self.weight_regularizer_l1>0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0 ] = -1\n",
    "            self.dweights += self.weights_regularizer_l1*dl1\n",
    "\n",
    "        if self.weight_regularizer_l2>0:\n",
    "            self.dweights += 2*self.weight_regularizer_l2*self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1>0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases<0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1*dl1\n",
    "\n",
    "        if self.bias_regularizer_l2>0:\n",
    "            self.dbiases += 2*self.bias_regularizer_l2*self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "\n",
    "    def set_parameters(self,weights,biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "class Layer_dropout:\n",
    "    def __init__ (self, rate):\n",
    "        self.rate = 1-rate\n",
    "\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.input = inputs\n",
    "        \n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        else:\n",
    "            self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape)/self.rate\n",
    "            self.output = inputs*self.binary_mask\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues*self.binary_mask\n",
    "\n",
    "class Layer_Input:\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "\n",
    "\n",
    "# ACTIVATION FUNCTIONS\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        if self.output is None:\n",
    "            raise ValueError(\"Sigmoid backward: self.output is None; forward pass may not have been called correctly\")\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        return self.dinputs\n",
    "\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "#OPTIMIZER\n",
    "             \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1., decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rate*layer.dweights\n",
    "            layer.weights_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum*layer.bias_momentums - self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate*layer.dweights\n",
    "            bias_updates = -self.current_learning_rate*layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates \n",
    "    \n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "#LOSS\n",
    "\n",
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights*layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.bias))\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases*layer.biases)\n",
    "    \n",
    "        return regularization_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers  = trainable_layers\n",
    "\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Loss_BinaryCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        sample_losses = -(y_true*np.log(y_pred_clipped)  +  (1-y_true)*np.log(1-y_pred_clipped))\n",
    "\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "\n",
    "        self.dinputs = -(y_true/clipped_dvalues - (1-y_true)/(1-clipped_dvalues))  / outputs\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "\n",
    "\n",
    "#ACCURACY\n",
    "\n",
    "\n",
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        accuracy = np.mean(comparisons)\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_accumulated(self):\n",
    "        if self.accumulated_count == 0:\n",
    "            return 0  # Avoid division by zero\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Accuracy_Binary(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.new_pass()  # Initialize counters at creation\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        # Threshold sigmoid outputs at 0.5\n",
    "        binary_predictions = predictions > 0.5\n",
    "        # Ensure y is flat and binary (not one-hot or 2D with single column)\n",
    "        if len(y.shape) == 2 and y.shape[1] == 1:\n",
    "            y = y.flatten()\n",
    "        return binary_predictions == y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL OBJECT\n",
    "\n",
    "class funny_guy:\n",
    "    def __init__ (self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        self.trainable_layers = []\n",
    "\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            elif i<layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            else:\n",
    "               self.layers[i].prev = self.layers[i-1] \n",
    "               self.layers[i].next = self.loss\n",
    "               self.output_layer_activation = self.layers[i]\n",
    "    \n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters=[]\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            return parameters\n",
    "\n",
    "\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "\n",
    "\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        validation_steps = 1\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val)//batch_size\n",
    "\n",
    "            if validation_steps*batch_size<len(X_val):\n",
    "                validation_steps+=1\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "            else:\n",
    "                batch_X = X_val[step*batch_size:(step+1)*batch_size]\n",
    "                batch_y = y_val[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "            output = self.forward(batch_X, training=False)\n",
    "            self.loss.calculate(output, batch_y)\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        print(f'validation, ' + f'acc:{validation_accuracy:.3f} ' + f'loss:{validation_loss:.3f}')   \n",
    "   \n",
    "    \n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "        output = self.input_layer.output  # Start with input layer's output\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, training)\n",
    "        return output\n",
    "\n",
    "    def backward(self, output, y):\n",
    "        dvalues = self.loss.backward(output, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            dvalues = layer.backward(dvalues)\n",
    "\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):\n",
    "        train_steps = 1\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f'epoch: {epoch}')\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            for step in range(train_steps):\n",
    "                batch_X = X if batch_size is None else X[step * batch_size:(step + 1) * batch_size]\n",
    "                batch_y = y if batch_size is None else y[step * batch_size:(step + 1) * batch_size]\n",
    "\n",
    "                output = self.forward(batch_X, training=True)\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, acc: {accuracy:.3f}, loss: {loss:.3f}, '\n",
    "                          f'(data_loss: {data_loss}, reg_loss: {regularization_loss:.3f}), '\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "    def predict(self, X, *, batch_size=None):\n",
    "        prediction_steps =1\n",
    "        if batch_size is not None:\n",
    "            prediction_steps = len(X) // batch_size\n",
    "            if prediction_steps*batch_size<len(X):\n",
    "                prediction_steps+=1\n",
    "    \n",
    "        output = []\n",
    "        for step in range(prediction_steps):\n",
    "            if batch_size is None:\n",
    "                batch_X = X\n",
    "            else:\n",
    "                batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "            batch_output = self.forward(batch_X, training=False)\n",
    "            batch_predictions = self.output_layer_activation.predictions(batch_output)\n",
    "            output.append(batch_output)\n",
    "\n",
    "        return np.vstack(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_parameters(self, path):\n",
    "        with open(path, 'wb') as f: #write binary\n",
    "            pickle.dump(self.get_parameters(),f)\n",
    "\n",
    "\n",
    "    def load_parameters(self, path):\n",
    "        with open(path, 'rb') as f: #read binary\n",
    "            self.set_parameters(pickle.load(f))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        model = copy.deepcopy(self)\n",
    "\n",
    "        model.loss.new_pass()\n",
    "        model.accuracy.new_pass()\n",
    "\n",
    "        model.input_layer.__dict__.pop('output', None)\n",
    "        model.loss.__dict__.pop('dinputs', None)\n",
    "\n",
    "        for layer in model.layers:\n",
    "            for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "                layer.__dict__.pop(property, None)\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            return model\n",
    "\n",
    "\n",
    "\n",
    "# Generating the Dataset from Pubmed\n",
    "\n",
    "def fetch_pubmed_ids(query, max_results=20):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    search_url = f\"{base_url}esearch.fcgi?db=pubmed&term={query}&retmax={max_results}&retmode=xml\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch IDs: {response.status_code}\")\n",
    "    root = ET.fromstring(response.content)\n",
    "    return [id_elem.text for id_elem in root.findall(\".//Id\")]\n",
    "\n",
    "def fetch_pubmed_details(pmids):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    fetch_url = f\"{base_url}efetch.fcgi?db=pubmed&id={','.join(pmids)}&retmode=xml\"\n",
    "    response = requests.get(fetch_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch details: {response.status_code}\")\n",
    "    fetch_root = ET.fromstring(response.content)\n",
    "    articles = {}\n",
    "    for article in fetch_root.findall(\".//PubmedArticle\"):\n",
    "        pmid = article.findtext(\".//PMID\")\n",
    "        title = article.findtext(\".//ArticleTitle\") or \"No title\"\n",
    "        abstract = article.findtext(\".//AbstractText\") or \"\"\n",
    "        articles[pmid] = {'title': title, 'abstract': abstract}\n",
    "    return articles\n",
    "\n",
    "def get_ollama_embedding(text, model='nomic-embed-text'):\n",
    "    response = ollama.embeddings(model=model, prompt=text)\n",
    "    return np.array(response['embedding'])\n",
    "\n",
    "def process_articles(pmids, articles_dict):\n",
    "    X, y, texts = [], [], []\n",
    "    for pmid in pmids:\n",
    "        article = articles_dict[pmid]\n",
    "        text = f\"{article['title']} {article['abstract']}\"\n",
    "        embedding = get_ollama_embedding(text)\n",
    "        X.append(embedding)\n",
    "        texts.append(text)\n",
    "    X = np.array(X)\n",
    "    print(\"Label the following articles (1 = useful, 0 = not useful):\")\n",
    "    # How many characters do are needed in the snippet\n",
    "    y = [int(input(f\"PMID {pmid}: {texts[i][:250]}...\\nLabel (1/0): \")) for i, pmid in enumerate(pmids)]   \n",
    "    return X, np.array(y)\n",
    "\n",
    "def create_article_data(query, max_results=20):\n",
    "    all_pmids = fetch_pubmed_ids(query, max_results)\n",
    "    train_pmids, test_pmids = train_test_split(all_pmids, test_size=0.2, random_state=42)\n",
    "    train_articles = fetch_pubmed_details(train_pmids)\n",
    "    test_articles = fetch_pubmed_details(test_pmids)\n",
    "    X_train, y_train = process_articles(train_pmids, train_articles)\n",
    "    X_test, y_test = process_articles(test_pmids, test_articles)\n",
    "    assert not set(train_pmids) & set(test_pmids), \"Overlap detected!\"\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label the following articles (1 = useful, 0 = not useful):\n",
      "Label the following articles (1 = useful, 0 = not useful):\n",
      "epoch: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,4) and (1,64) not aligned: 4 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train with validation\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test[:\u001b[38;5;241m5\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 129\u001b[0m, in \u001b[0;36mfunny_guy.train\u001b[1;34m(self, X, y, epochs, batch_size, print_every, validation_data)\u001b[0m\n\u001b[0;32m    126\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_activation\u001b[38;5;241m.\u001b[39mpredictions(output)\n\u001b[0;32m    127\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy\u001b[38;5;241m.\u001b[39mcalculate(predictions, batch_y)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mpre_update_params()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_layers:\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mfunny_guy.backward\u001b[1;34m(self, output, y)\u001b[0m\n\u001b[0;32m    103\u001b[0m dvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward(output, y)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 105\u001b[0m     dvalues \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mLayer_Dense.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_regularizer_l2\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbiases \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_regularizer_l2\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,4) and (1,64) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    query = \"heart disease and diabetes\"  # Query topic\n",
    "    X_train, y_train, X_test, y_test = create_article_data(query, max_results=20)\n",
    "\n",
    "    model = funny_guy()\n",
    "    model.add(Layer_Dense(768, 64))\n",
    "    model.add(Activation_ReLU())\n",
    "    model.add(Layer_Dense(64, 1))\n",
    "    model.add(Activation_Sigmoid())\n",
    "    model.set(\n",
    "        loss=Loss_BinaryCrossEntropy(),\n",
    "        optimizer=Optimizer_SGD(learning_rate=0.1, decay=1e-3),\n",
    "        accuracy=Accuracy_Binary()\n",
    "    )\n",
    "    model.finalize()\n",
    "\n",
    "    # Train with validation\n",
    "    model.train(X_train, y_train, epochs=10, batch_size=4, print_every=2, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(X_test[:5])\n",
    "    print(\"Predictions:\", predictions.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
